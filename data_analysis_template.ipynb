{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "plt.rcParams['figure.figsize'] = [8,5]\n",
    "plt.rcParams['font.size'] =14\n",
    "plt.rcParams['font.weight']= 'bold'\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# creating a function to create adhusted R-Squared\n",
    "\n",
    "def adj_r2(X, y, model):\n",
    "    r2 = model.score(X, y)\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "    return adjusted_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "df = pd.read_csv('./train.csv')\n",
    "print('\\nNumber of rows and columns in the data set: ', df.shape)\n",
    "df.head()\n",
    "# Drop useless columns like id\n",
    "# df = df.drop([\"id\"], axis = 1)\n",
    "\n",
    "# Basic information\n",
    "df.describe()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset has empty, zero values or duplicates\n",
    "# https://www.kaggle.com/discussions/getting-started/250322\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.heatmap(df.isnull(),cbar=False,cmap='viridis',yticklabels=False)\n",
    "plt.title('Missing value in the dataset')\n",
    "# Empty entries by feature\n",
    "df.isna().sum()\n",
    "\n",
    "# Ways of handling missing data\n",
    "# filling missing value using fillna()   \n",
    "df.fillna(0) \n",
    "# filling a missing value with previous ones   \n",
    "df.fillna(method ='ffill') \n",
    "# fill with next ones\n",
    "df.fillna(method ='bfill') \n",
    "# filling a null values with string\n",
    "df[\"Gender\"].fillna(\"No Gender\", inplace = True)  \n",
    "# will replace Nan value in dataframe with value -99   \n",
    "df.replace(to_replace = np.nan, value = -99)  \n",
    "\n",
    "# to interpolate the missing values  \n",
    "df.interpolate(method ='linear', limit_direction ='forward') \n",
    "df.interpolate(method='polynomial', order=5)\n",
    "# interpolate for time series\n",
    "df.interpolate('time')\n",
    "\n",
    "# drop rows / columns if anything is missing   \n",
    "df.dropna() # df.dropna(axis = 1) \n",
    "# drop rows only if all are missing    \n",
    "df.dropna(how = 'all') \n",
    "\n",
    "\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated()\n",
    "df[duplicates]\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the count of zeroes per column\n",
    "zero_count = (df == 0).sum()\n",
    "\n",
    "# Calculate the percentage of zeroes per column\n",
    "zero_percentage = (df == 0).mean() * 100\n",
    "\n",
    "# Print the count and percentage of zeroes\n",
    "print(\"Count of zeroes per column:\\n\", zero_count)\n",
    "print(\"\\nPercentage of zeroes per column:\\n\", zero_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with Outliers with IQR\n",
    "\n",
    "# column_list = [\"list of columns we wish to crop outliers\"]\n",
    "column_list = [\"allelectrons_Total\"]\n",
    "df_outliers = pd.DataFrame(df.loc[:,column_list])\n",
    "\n",
    "# Calculate IQR\n",
    "Q1 = df_outliers.quantile(0.25)\n",
    "Q3 = df_outliers.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Method 1. Set extreme values to some maximum/min\n",
    "    # We can use IQR score to filter out the outliers by keeping only valid values\n",
    "    # Replace every outlier on the upper side by the upper whisker\n",
    "    print(\"Data which is outside 1.5 IQR:\")\n",
    "\n",
    "    whisker = Q3 + 1.5 * IQR\n",
    "    for i, j in zip(np.where(df_outliers > whisker)[0], np.where(df_outliers > whisker)[1]):\n",
    "        \n",
    "        print(f\"Row {i}  {column_list[j]}  {df_outliers.iloc[i,j]}\")\n",
    "\n",
    "        df_outliers.iloc[i,j] = whisker.iloc[j]\n",
    "        \n",
    "    # Replace every outlier on the lower side by the lower whisker\n",
    "    whisker  = Q1 - 1.5 * IQR\n",
    "    for i, j in zip(np.where(df_outliers < whisker)[0], np.where(df_outliers < whisker)[1]): \n",
    "        df_outliers.iloc[i,j] = whisker.iloc[j]\n",
    "    \n",
    "    # Combine back\n",
    "    df_corrected = df.drop(column_list, axis = 1)\n",
    "    df_corrected = pd.concat([df_outliers, df], axis = 1)\n",
    "\n",
    "# Method 2: delete rows with any outliers\n",
    "    row_list = ((df_outliers >= (Q1 - 1.5 * IQR)) & (df_outliers <= (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "    df_corrected = df_outliers[row_list]\n",
    "\n",
    "# once sure then overwrite\n",
    "df = df_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: adding new features\n",
    "# https://www.kaggle.com/code/vinayakshanawad/random-forest-with-bootstrap-sampling-for-beginner \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualisation with various plots\n",
    "\n",
    "X = df.drop('Hardness', axis = 1)\n",
    "y = df['Hardness']\n",
    "# Violin Plot / sns.boxplot\n",
    "    # inner=\"quart\" or \"box\"\n",
    "    # hue=\"diagnosis\" (categorical result variable we care about)\n",
    "\n",
    "    # Just showing the histograms without indication of result variable\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.violinplot(data = X)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "    # For continous response variable\n",
    "    # For a feature with categorical data (M/F, 0/1/2/3) , show them side by side for each category\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.violinplot(x=\"feature\", y=\"Hardness\", data=df, split=True) \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "    # For Yes/no response variable \"diagnosis\", show two histograms side by side\n",
    "    # This shows if this feature would be a good predictor, if the two distributions are very different\n",
    "    X_norm = (X - X.mean()) / (X.std()) \n",
    "    data = pd.concat([y,X_norm],axis=1)\n",
    "    data = pd.melt(data,id_vars=\"diagnosis\",\n",
    "                        var_name=\"features\",\n",
    "                        value_name='value')\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data, split=True)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Joint plot\n",
    "    # Essentially linear regression + histogram of two features with pearson's R correlation statistic\n",
    "    sns.jointplot(x = X.loc[:,'feature1'], y = X.loc[:,'feature2'], kind=\"reg\", color=\"#ce1414\")\n",
    "\n",
    "# Pair plot\n",
    "    # Scatter plot/histogram of pairwise features\n",
    "    sns.pairplot(X, diag_kind = 'kde', kind = \"reg\", corner = True) # or use df\n",
    "\n",
    "# Scatter plot against response\n",
    "    plt.figure(figsize = (20, 15))\n",
    "    plotnumber = 1\n",
    "\n",
    "    for column in df:\n",
    "        if plotnumber <= 14:\n",
    "            ax = plt.subplot(3, 5, plotnumber)\n",
    "            sns.scatterplot(x = df[column], y = df['Hardness'])\n",
    "            \n",
    "        plotnumber += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection on Numerical Columns\n",
    "\n",
    "# X = X.loc[:,column_list] # pick out the numerical columns\n",
    "X = (X - X.mean()) / (X.std()) \n",
    "\n",
    "# correlation plot\n",
    "    corr = df.corr()\n",
    "    sns.heatmap(corr, cmap = 'Wistia', annot= True)\n",
    "    # Look by eye which rows look the same\n",
    "\n",
    "# checking for multicollinearity using VIF\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    vif = pd.DataFrame()\n",
    "    vif['VIF'] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "    vif['Features'] = X.columns\n",
    "    vif\n",
    "    # VIF = 1/(1-r^2) where r^2 is the result of regressing this feature onto the remaining ones\n",
    "    # VIF = 1: no correlation\n",
    "    # VIF > 10: high correlation, consider dropping it\n",
    "    # drop column and re-run\n",
    "\n",
    "# PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "    # PCA gives an idea of how many features are relevant, but best not to use those combinations since it is hard to explain\n",
    "    # No need to split train and test if just getting an idea\n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "    component_names = [f\"PC{i+1}\" for i in range(X.shape[1])]\n",
    "    X_pca = pd.DataFrame(pca.components_, columns=component_names, index=X.columns)\n",
    "    X_pca.head()\n",
    "    # If we think this is a reasonable combination of stuff, we should pca.transform(X)\n",
    "\n",
    "    plt.figure(1, figsize=(14, 13))\n",
    "    plt.clf()\n",
    "    plt.axes([.2, .2, .7, .7])\n",
    "    plt.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "    plt.axis('tight')\n",
    "    plt.xlabel('n_components')\n",
    "    plt.ylabel('explained_variance_ratio_')\n",
    "\n",
    "\n",
    "\n",
    "# several methods of feature selection\n",
    "# https://www.kaggle.com/code/kanncaa1/feature-selection-and-data-visualization\n",
    "\n",
    "# sklearn Select K best features\n",
    "    from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "    # f_regression computes the F-test ANOVA score for a linear regression between feature and target\n",
    "    # mutual_info_regression computes score between features\n",
    "    # for classification task, use f_classif, mutual_info_classif\n",
    "\n",
    "    select_feature = SelectKBest(f_regression, k=5).fit(X, y)\n",
    "    feature_scores = pd.DataFrame({\"Features\": X.columns,\n",
    "                \"Score\": select_feature.scores_\n",
    "                })\n",
    "    print(feature_scores)\n",
    "\n",
    "    # Reduce x_train to the k best features\n",
    "    X = select_feature.transform(X)\n",
    "\n",
    "# See Random Forest and XGBoost as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising data for regression and splitting into train and test\n",
    "# https://www.kaggle.com/code/humvale/sklearn-numerical-and-categorical-pipeline\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separating data and target\n",
    "X = df.drop(columns = ['Hardness'], axis = 1) # and drop any other columns from previous feature selection\n",
    "y = df['Hardness']\n",
    "\n",
    "# if the target y is categorical, then replace it with 0 ~ n-1. If not then comment out the below\n",
    "# But DO NOT APPLY REGRESSION to this, and separate it out from the selector below\n",
    "y_preprocessor = LabelEncoder()\n",
    "y_preprocessor.fit_transform(y)\n",
    "\n",
    "# Now deal with X\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(df)\n",
    "categorical_columns = categorical_columns_selector(df)\n",
    "\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\") # if it sees a new category, it gets all 0s without being in a new category\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
    "    ('standard-scaler', numerical_preprocessor, numerical_columns)])\n",
    "\n",
    "# split data train 70 % and test 30 %\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "x_train = preprocessor.fit_transform(x_train)\n",
    "x_test = preprocessor.transform(x_test)\n",
    "x_train = pd.DataFrame(x_train, columns=X.columns)\n",
    "x_test = pd.DataFrame(x_test, columns=X.columns)\n",
    "\n",
    "# If y is numerical then transform it also\n",
    "y_train = preprocessor.fit_transform(y_train)\n",
    "y_test = preprocessor.transform(y_test)\n",
    "y_train = pd.DataFrame(y_train, columns=y.columns)\n",
    "y_test = pd.DataFrame(y_test, columns=y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "# Use this to:\n",
    "# 1. estimate how many features are important\n",
    "# 2. This is also a predictor\n",
    "\n",
    "# Inputs\n",
    "# n_estimators: numbers of trees, more is better but slower\n",
    "# bootstrap: True\n",
    "# max_samples: The fraction of samples to draw from X to train each tree [0,1]\n",
    "# Bagging fraction = (1-1/N)^N'=e^(-max_samples)\n",
    "# max_features: proportion of total features to use, try sqrt or log2\n",
    "# max_depth: too much of this causes over fitting\n",
    "# min_samples_split: causes underfitting\n",
    "# min_samples_leaf: some value > 1 is slightly better, after that underfits\n",
    "\n",
    "# Classification Task\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators = 100, random_state=43, max_features=\"log2\", min_samples_leaf=3,max_depth=5)      \n",
    "    rf.fit(x_train,y_train)\n",
    "\n",
    "    ac = accuracy_score(y_test,rf.predict(x_test))\n",
    "    print('Accuracy is: ',ac)\n",
    "    cm = confusion_matrix(y_test,rf.predict(x_test))\n",
    "    sns.heatmap(cm,annot=True,fmt=\"d\")\n",
    "\n",
    "# Continous Output: RF Regressor\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    rf = RandomForestRegressor(n_estimators = 100, random_state = 42, max_features=\"log2\", min_samples_leaf=3,max_depth=5)\n",
    "    rf.fit(x_train, y_train)\n",
    "    print(rf.score(x_train, y_train))\n",
    "    print(rf.score(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neighbours\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "# https://www.kaggle.com/code/mjamilmoughal/k-nearest-neighbor-classifier-to-predict-fruits\n",
    "# use classifier / regressor\n",
    "# n_neighbors: vary this to find optimal value\n",
    "# p uses Lp norm as distance\n",
    "scores = pd.DataFrame(columns=[\"test_score\"])\n",
    "for k in range(5, 40):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, weights = 'distance',p = 2)\n",
    "    knn.fit(x_train,y_train)\n",
    "    scores.loc[k,'test_score'] = knn.score(x_test,y_test)\n",
    "\n",
    "scores.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression OLS (Detailed)\n",
    "\n",
    "# With detailed report\n",
    "import statsmodels.formula.api as smf\n",
    "# formula = 'y ~ x' as column names\n",
    "lm = smf.ols(formula = 'MEDV ~ RAD', data = df).fit()\n",
    "lm.summary()\n",
    "\n",
    "# Basic version\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "lr.score(x_train, y_train)\n",
    "lr.score(x_test, y_test)\n",
    "print(adj_r2(x_train, y_train, lr))\n",
    "print(adj_r2(x_test, y_test, lr))\n",
    "y_pred = lr.predict(x_test)\n",
    "coefficients = pd.DataFrame(lr.coef_,index = x_train.columns,columns=[\"coef\"])\n",
    "coefficients.sort_values(by=\"coef\", ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# https://www.kaggle.com/code/prashant111/logistic-regression-classifier-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO, Ridge, Elastic Net with CV\n",
    "# Description: https://www.kaggle.com/code/shweta2407/how-to-choose-best-regression-model \n",
    "# Detailed alpha search: https://www.kaggle.com/code/fugacity/ridge-lasso-elasticnet-regressions-explained\n",
    "# LASSO\n",
    "    from sklearn.linear_model import Lasso, LassoCV\n",
    "\n",
    "    lasso_cv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
    "    lasso_cv.fit(x_train, y_train)\n",
    "\n",
    "    # best alpha parameter\n",
    "    alpha = lasso_cv.alpha_\n",
    "    lasso = Lasso(alpha = lasso_cv.alpha_)\n",
    "    lasso.fit(x_train, y_train)\n",
    "    lasso.score(x_train, y_train)\n",
    "    lasso.score(x_test, y_test)\n",
    "    print(adj_r2(x_train, y_train, lasso))\n",
    "    print(adj_r2(x_test, y_test, lasso))\n",
    "\n",
    "    # To investigate how the fitting happened, use LARS\n",
    "    from sklearn.linear_model import lars_path\n",
    "    alphas, _, coefs = lars_path(x_train, y_train, method='lasso', verbose=True)\n",
    "\n",
    "    xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "    xx /= xx[-1]\n",
    "\n",
    "    plt.plot(xx, coefs.T)\n",
    "    ymin, ymax = plt.ylim()\n",
    "    plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "    plt.xlabel('|coef| / max|coef|')\n",
    "    plt.ylabel('Coefficients')\n",
    "    plt.title('LASSO Path')\n",
    "    plt.axis('tight')\n",
    "    plt.show()\n",
    "\n",
    "# Ridge Regression\n",
    "    from sklearn.linear_model import Ridge, RidgeCV\n",
    "\n",
    "    alphas = np.random.uniform(0, 10, 50)\n",
    "    ridge_cv = RidgeCV(alphas = alphas, cv = 10, normalize = True)\n",
    "    ridge_cv.fit(x_train, y_train)\n",
    "    # best alpha parameter\n",
    "    alpha = ridge_cv.alpha_\n",
    "    ridge = Ridge(alpha = ridge_cv.alpha_)\n",
    "    ridge.fit(x_train, y_train)\n",
    "    ridge.score(x_train, y_train)\n",
    "    ridge.score(x_test, y_test)\n",
    "    print(adj_r2(x_train, y_train, ridge))\n",
    "    print(adj_r2(x_test, y_test, ridge))\n",
    "\n",
    "    # to see the regularization path:\n",
    "    from sklearn.linear_model import LassoLarsCV\n",
    "    lassolarscv = LassoLarsCV().fit(x_train, y_train)\n",
    "    coef_path = pd.DataFrame(lassolarscv.coef_path_,index=x_train.columns).T\n",
    "    coef_path[\"coef_l2\"] = np.sqrt(np.square(coef_path).sum(axis=1))\n",
    "    coef_max = coef_path[\"coef_l2\"].max()\n",
    "    coef_path[\"coef_ratio\"] = coef_path[\"coef_l2\"]/coef_max\n",
    "    plt.plot(coef_path[\"coef_ratio\"], coef_path.iloc[:,0:7])\n",
    "    plt.show()\n",
    "\n",
    "# Elastic Net\n",
    "    from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "\n",
    "    elastic_net_cv = ElasticNetCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
    "    elastic_net_cv.fit(x_train, y_train)\n",
    "\n",
    "    alpha = elastic_net_cv.alpha_\n",
    "    # l1 ratio \n",
    "    elastic_net_cv.l1_ratio\n",
    "\n",
    "    elastic_net = ElasticNet(alpha = elastic_net_cv.alpha_, l1_ratio = elastic_net_cv.l1_ratio)\n",
    "    elastic_net.fit(x_train, y_train)\n",
    "    elastic_net.score(x_train, y_train)\n",
    "    elastic_net.score(x_test, y_test)\n",
    "    print(adj_r2(x_train, y_train, elastic_net))\n",
    "    print(adj_r2(x_test, y_test, elastic_net))\n",
    "\n",
    "# Show weights\n",
    "    import eli5\n",
    "    for model in [lasso, ridge, elastic_net]:\n",
    "        print(f\"model: {model}\")\n",
    "        eli5.show_weights(model, top=-1, feature_names = x_train.columns.tolist())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Ridge\n",
    "# https://www.kaggle.com/code/kzhoulatte/kernel-ridge-regression-lb0-0571-rbf-laplacian \n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import polynomial_kernel, rbf_kernel, laplacian_kernel\n",
    "\n",
    "kernel_list = ['linear', 'polynomial', 'rbf', 'laplacian']\n",
    "for kernel in kernel_list:\n",
    "    model = KernelRidge(kernel ='linear', alpha=1.0)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "    print(f\"Kernel {kernel} RMSE {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust regression: Theil-sen, RANSAC, Huber\n",
    "# These are better at dealing with outliers\n",
    "# https://www.kaggle.com/code/plarmuseau/better-regression-techniques \n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor, TheilSenRegressor, HuberRegressor\n",
    "\n",
    "estimators = [('OLS', LinearRegression()),\n",
    "              ('Theil-Sen', TheilSenRegressor(random_state=42)),\n",
    "              ('RANSAC', RANSACRegressor(random_state=42)),\n",
    "              ('HuberRegressor', HuberRegressor())]\n",
    "for name, estimator in estimators:\n",
    "    estimator.fit(x_train, y_train)\n",
    "    print(name,estimator.score(x_train, y_train))\n",
    "    print(name,estimator.score(x_test, y_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# https://www.kaggle.com/code/stuarthallows/using-xgboost-with-scikit-learn\n",
    "import xgboost as xgb\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "\n",
    "# objective: \"reg:linear\"  \"binary:logistic\"  \"multi:softprob\"\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:linear\", random_state=42)\n",
    "\n",
    "xgb_model.fit(x_train, y_train)\n",
    "y_pred = xgb_model.predict(x_test)\n",
    "rmse=np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "# Cross validation scores\n",
    "scores = cross_val_score(xgb_model, X, y, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "print(f\"Scores: {np.sqrt(-scores)}\")\n",
    "\n",
    "# Hyperparameter Searching\n",
    "# objective: \"reg:linear\"  \"binary:logistic\"  \"multi:softprob\"\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:linear\", random_state=42)\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": randint(100, 150), # default 100\n",
    "    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "    \"max_depth\": randint(2, 6), # default 3\n",
    "    \"subsample\": uniform(0.6, 0.4),\n",
    "    \"colsample_bytree\": uniform(0.7, 0.3),\n",
    "    \"gamma\": uniform(0, 0.5),\n",
    "    \"alpha\": uniform(0, 10),\n",
    "    \n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=10, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "search.fit(x_train, y_train)\n",
    "results = search.cv_results_\n",
    "candidates = np.flatnonzero(results['rank_test_score'] == 1)\n",
    "for candidate in candidates:\n",
    "    print(\"Best Models\")\n",
    "    print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "            results['mean_test_score'][candidate],\n",
    "            results['std_test_score'][candidate]))\n",
    "    print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "    print(\"\")\n",
    "\n",
    "# Then use these parameters to define a newxgb_model again\n",
    "\n",
    "# Early stopping\n",
    "xgb_model.fit(x_train, y_train, early_stopping_rounds=10, eval_set=[(x_test, y_test)], verbose=False)\n",
    "xgb.plot_importance(xgb_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with Cross-Validation\n",
    "# https://www.kaggle.com/code/prashant111/xgboost-k-fold-cv-feature-importance \n",
    "# This uses decision trees and adds them one at a time\n",
    "# Using train-test split, this tells us if XGboost is a good model or not\n",
    "# Once we are happy, we do CV on the entire dataset to get a slightly better model for future predictions\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, cv\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=x_train,label=y_train)\n",
    "# data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "params = {\"objective\":\"binary:logistic\", # \"reg:squarederror\"\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 5, \n",
    "          'alpha': 10}\n",
    "\n",
    "xgb_cv = cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50, early_stopping_rounds=10, metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "my_model = XGBRegressor(n_estimators=1000)\n",
    "# n_estimators specifies how many times to go through the modeling cycle\n",
    "# verbose = print messages\n",
    "my_model.fit(x_train, y_train, verbose=False )\n",
    "print('XGBoost model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As I understand, you first use xgb.cv (the cross validation function of XGB) to \"tune\" the parameters you will use in your model (mainly the optimal nrounds).\n",
    "# Once you had found the optimal parameters for your dataset (that gave you the minimal logloss/error) you then replicate this parameters in a training function of the XGBoost package, that are xgboost or xgb.train to obtain the model you can use to predict on the test set.\n",
    "    \n",
    "# XGBoost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# declare parameters\n",
    "params = {\n",
    "            'objective':'binary:logistic',\n",
    "            'max_depth': 4,\n",
    "            'alpha': 10,\n",
    "            'learning_rate': 1.0,\n",
    "            'n_estimators':100\n",
    "        }         \n",
    "           \n",
    "# instantiate the classifier \n",
    "xgb_clf = XGBClassifier(**params)\n",
    "# fit the classifier to the training data\n",
    "xgb_clf.fit(x_train, y_train)\n",
    "# we can view the parameters of the xgb trained model\n",
    "print(xgb_clf)\n",
    "# make predictions on test data\n",
    "y_pred = xgb_clf.predict(x_test)\n",
    "# compute and print accuracy score\n",
    "print('XGBoost model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "# See the importance of various features\n",
    "xgb.plot_importance(xgb_clf)\n",
    "plt.figure(figsize = (16, 12))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# import XGBoost\n",
    "import xgboost as xgb\n",
    "# define data_dmatrix\n",
    "from xgboost import cv\n",
    "\n",
    "params = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "\n",
    "xgb_cv = cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50, early_stopping_rounds=10, metrics=\"auc\", as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML\n",
    "# https://www.kaggle.com/code/dimitreoliveira/deep-learning-for-time-series-forecasting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
